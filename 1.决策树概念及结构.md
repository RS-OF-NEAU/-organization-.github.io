### 1.决策树概念及结构

决策树算法是以树状结构表示数据分类的结果，每一个决策点实现一个具有离散输出的测试，如图所示

首先决策树里边有一个根节点，就是数据放决策树当中，它是先走的那一层决策， 。我们说，它是一个根节点。

还有什么非叶子节点飞叶子节点的意思！就是说对于这个节点，他下面还是有一个分支！还是有个分之说明这个。节点还没有结束，把它没有得出来一个最终的结果，直我们把这样的节点叫做一个非叶子节点。

那么，什么叫做叶子节点？这样最后我们看最后一个节点。最后这些是不是他就是最后一个，所以说，我们说最后的这样底下不再进行一个分裂了，我们就把这些点叫做一个叶子节点。其实，这些叶子节点里边包含的就是最终想要的一个结果值了。把每一个叶子结点里边都有一个唯一的类别是有这样一个回归，只可以计算出来，

分支就是这样一个杈了，这样一个杈了这样一个杈，那么在这个决策树当中，现在来说一下有这样一个根节点也有非叶子叶子节点还有分支，还有叶节点。

 

![img](file:///C:/Users/崔红涛/AppData/Local/Temp/msohtmlclip1/01/clip_image002.jpg)

举一个例子来说，假如我要在如图以下五个人中找出一个会打篮球的人。我们发现，这五个人分别是爷爷·奶奶·妈妈·女儿·儿子，我们假定年轻而且是男性的人会打篮球。我可以通过他们的年级是否大于15岁，找出第一组符合年龄条件的人，在通过是男生还是女生选出第二组符合性别条件的人，于是，我们通过两次决策来选出所需要的选择。

![img](file:///C:/Users/崔红涛/AppData/Local/Temp/msohtmlclip1/01/clip_image004.jpg)

2.熵及信息增益

当然我们也可以通过先选择是男性在选择小于15岁的挑选出需要的人从而做出决策。到底哪种方式好呢，这时候我们就可以引入熵这个概念

A熵

熵通俗的讲是就是混乱程度，这里我们称不确定性程度。

但是通过调查，我们可以把不确定变为确定，我们把消除不确定的东西成为信息。

从定义**来讲：** 当一件事情有多种可能情况时，这件事情对某人而言具体是哪种情况的不确定性叫做**熵**，而能够消除该人对这件事情不确定性的事物叫做**信息**。

 

**熵和信息的关系：**数量相等，意义相反。获取信息 = 消除熵。

 

对应的我们把不能消除不确定的东西叫做噪声，通常来说我们进行决策依赖的数据，既包含信息，也包含噪声。这时候我们就要对不确定性程度进行量化，以得知数据的准确程度

我们可以这样想，

我们投1个硬币，他有2种情况

我们投2个硬币，他有4种情况

我们投3个硬币，他有8种情况

我们投m个硬币，他有![img](file:///C:/Users/崔红涛/AppData/Local/Temp/msohtmlclip1/01/clip_image006.png)种情况

……

我们也可以这样说，

- 定义：抛1次硬币的不确定性为1Bit，出现结果为2种等概率事件(这里的1Bit就是单位信息量)。
- 那么，抛2次硬币的不确定性为2Bit,出现结果为4种等概率事件，即（00/01/10/11）
- 那么，抛3次硬币的不确定性为3Bit,出现结果为8种等概率事件（000/001/010/011/100/101/110/111）。
- 那么，抛N次硬币的不确定性为NBit，出现结果为2^N=M种等概率事件，这里 因此，假设我们知道出现了16种等概率事件(即16种可能情况）,含有的信息量为     